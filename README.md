# Сервис-автоответчик на отзывы Wildberries с YandexGPT или OpenAI

Автоответчик, который регулярно подхватывает свежие отзывы из личного кабинета продавца Wildberries, прогоняет их через LLM (по умолчанию YandexGPT) и отправляет готовые ответы обратно через Feedbacks API. 
Проект рассчитан на запуск внутри Yandex Cloud Functions по расписанию, так что выделять и поддерживать собственный сервер не нужно.

## Кратко как всё устроено
- каждые 30 минут Cloud Function просыпается по cron-триггеру, забирает пачку необработанных отзывов через `wildberries/api/v1/feedbacks/list`;
- `PromptBuilder` собирает промпт с отзывом целиком, LLM выдаёт дружелюбный ответ в нужном языке;
- готовый текст публикуется через `wildberries/api/v1/feedbacks/answer`.

## Сколько это стоит?
- Yandex Cloud Functions тарифицируется по времени выполнения и объёму памяти. 
- YandexGPT тарифицируется по количеству обработанных токенов (входящих + выходящих).
- Wildberries Feedbacks API бесплатен в рамках лимитов личного кабинета продавца.
- На небольшой магазин с десятком отзывов в день, расходы будут минимальны ~ **100-200 рублей в месяц.**

## Что понадобится заранее
- Python 3.12+ и `pip` для локальных проверок;
- Node.js 18+ и `npm` — Serverless Framework подтягивается через `package.json`;
- установленный CLI `yc` с доступом к облаку и каталогу Yandex Cloud;
- токен Feedbacks API для вашего кабинета WB;
- папка в Яндекс Облаке с включённым доступом к YandexGPT (`ai.languageModels.user` на сервисный аккаунт выдаёт `serverless.yml`).

## Токен Wildberries
1. В личном кабинете продавца откройте Wildberries «Профиль → Настройки → Доступ к API → Отзывы и вопросы».
2. Создайте новый ключ и впишите его в `settings.yaml`:

```yaml
wildberries:
  api_token: "YOUR_WILDBERRIES_API_TOKEN"
  base_url: "https://feedbacks-api.wildberries.ru"
  request_timeout: 10
  batch_size: 10
```

`batch_size` задаёт сколько отзывов обрабатываем за вызов. При необходимости WB можно ограничивать чаще/реже за счёт cron-выражения в `serverless.yml`.

## Настройка Yandex Cloud и YandexGPT
### Установка и авторизация `yc`
```bash
curl https://storage.yandexcloud.net/yandexcloud-yc/install.sh | bash
yc init  # выбираем облако, каталог и авторизуемся по OAuth
```

### Достаём `folder_id`
```bash
yc resource-manager folder list
```
Скопируйте идентификатор нужной папки и подставьте в `settings.yaml`:
```yaml
llm:
  provider: "yandexgpt"
  yandexgpt:
    folder_id: "YOUR_YANDEX_CLOUD_FOLDER_ID"
    model: "aliceai-llm/latest"
```
Если запускаете код локально, добавьте сюда же `api_key`, созданный в консоли YandexGPT. 
В продакшене внутри Cloud Functions ключ не нужен: клиент автоматически запросит IAM-токен из метаданных и будет использовать его для вызова `rest-assistant.api.cloud.yandex.net`.

## npm-скрипты: установка, деплой и удаление
1. Установите JS-зависимости разом:
   ```bash
   npm install
   ```
   Здесь подтянется `serverless` и `@yandex-cloud/serverless-plugin`.
2. Сборка и деплой:
   ```bash
   serverless deploy
   ```
   Скрипт упакует `src/`, `requirements.txt` и `settings.yaml`, загрузит архив в Yandex Cloud и создаст Cloud Function `wb-responder-function` вместе с cron-триггером `*/30 * * * ? *`, который запускает обработку каждые 30 минут.
3. Удаление всех созданных ресурсов:
   ```bash
   serverless remove
   ```
   Команда снесёт функцию, сервисные аккаунты и триггер — полезно, если нужно быстро освободить лимиты.

## Конфигурация LLM
- Делайте правки через `settings.yaml`. Любую настройку можно перекрыть переменными окружения (Pydantic Settings позаботится об этом).
- Если захотите проверять другие модели, поменяйте `llm.provider` на `openai` и заполните `llm.openai.api_key` + `llm.openai.model`.
- Температуру, верхний лимит токенов, инструкции и шаблон промпта (`llm.prompt_template`) правим там же.

## Почему Cloud Functions (и почему без серверов)
Это по-настоящему serverless: функция холодно стартует по расписанию, берет IAM-токен из метаданных, вызывает YandexGPT и завершает работу. Никаких VM, Docker и обновлений ОС, только контейнер, который платно существует ровно столько, сколько длится обработка очередной пачки отзывов. Хотите чаще — подкручиваете cron, хотите реже — прописываете своё выражение вместо `*/30 * * * ? *`. А если вдруг нужно временно выключить автоматику, достаточно удалить или остановить триггер, функция останется в каталоге без расходов.

## Ещё немного полезного
- В `src/application/respond_on_reviews.py` всего одна оркестрация, так что при необходимости можно быстро подключить логирование метрик или ретраи.
- `tests/test_architecture.py` проверяет, что домен не зависит от инфраструктурных слоёв — удобно для быстрой регрессии.
- Любой кастомный промпт переносится через `settings.llm.prompt_template`. Если в нём требуется дополнительный контекст, добавьте это прямо в шаблон и перезапустите функцию.

### Понравился проект? Звёздочка на GitHub и фидбек в issues будут очень кстати!
